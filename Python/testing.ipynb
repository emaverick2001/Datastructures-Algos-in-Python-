{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8546d744",
   "metadata": {},
   "source": [
    "# Testing Patterns / Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72515197",
   "metadata": {},
   "source": [
    "## TDD Principles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cca493",
   "metadata": {},
   "source": [
    "- Software development practice that focuses on writing tests before writing the actual code.\n",
    "  \n",
    "- Red → Green → Refactor\n",
    "  \n",
    "- Red: Write a failing test that defines a small piece of desired functionality\n",
    "  - This confirms that the new feature or behavior doesn’t yet exist.\n",
    "\n",
    "- Green: Write just enough code to make the test pass.\n",
    "  - Don’t worry about perfection yet—just make the test succeed.\n",
    "\n",
    "- Refactor: Improve the existing code structure without changing behavior.\n",
    "  - Clean up duplication, improve readability, and ensure maintainability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6959dd",
   "metadata": {},
   "source": [
    "## AAAC: Arrange-Act-Assert-Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7805623",
   "metadata": {},
   "source": [
    "- Arrange\n",
    "  - where we prepare everything for our test. It’s lining up the dominoes so that the act can do its thing in one, state-changing step. \n",
    "  - Preparing objects, starting/killing services, entering records into a database, or even things like defining a URL to query, generating some credentials for a user that doesn’t exist yet, or just waiting for some process to finish.\n",
    "- Act\n",
    "  - singular, state-changing action that kicks off the behavior we want to test.\n",
    "  - behavior is what carries out the changing of the state of the system under test (SUT), and it’s the resulting changed state that we can look at to make a judgement about the behavior. \n",
    "  - typically takes the form of a function/method call.\n",
    "- Assert\n",
    "  - we look at that resulting state and check if it looks how we’d expect after the dust has settled.\n",
    "  - where we gather evidence to say the behavior does or does not align with what we expect. \n",
    "  - The assert in our test is where we take that measurement/observation and apply our judgement to it.\n",
    "- Cleanup\n",
    "  - where the test picks up after itself, so other tests aren’t being accidentally influenced by it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541075f",
   "metadata": {},
   "source": [
    "- Keep each phase visually separated (blank lines help)\n",
    "- Assert invariants (shape, monotonicity, conservation) not just values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(xs):\n",
    "    total = sum(xs)\n",
    "    return [x/total for x in xs] if total else xs\n",
    "\n",
    "# Arrange\n",
    "xs = [2, 2, 6]\n",
    "# Act\n",
    "ys = normalize(xs)\n",
    "# Assert\n",
    "assert abs(sum(ys) - 1.0) < 1e-9\n",
    "assert len(ys) == len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e200e3",
   "metadata": {},
   "source": [
    "## FIRST Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b41a9",
   "metadata": {},
   "source": [
    "- A set of guidelines to make tests effective and reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b5591",
   "metadata": {},
   "source": [
    "| Letter | Principle      | Meaning                                                        |\n",
    "|--------|---------------|----------------------------------------------------------------|\n",
    "| F      | Fast          | Tests should run quickly to encourage frequent runs.            |\n",
    "| I      | Independent   | Tests should not depend on each other.                          |\n",
    "| R      | Repeatable    | Tests should produce the same result every time.                |\n",
    "| S      | Self-Validating | Tests should have clear pass/fail outcomes (no print statements). |\n",
    "| T      | Timely        | Write tests at the right time — before or alongside code.       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3babed7",
   "metadata": {},
   "source": [
    "## Isolation Principle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ba8da3",
   "metadata": {},
   "source": [
    "- Each test should verify one unit of behavior in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolation Principle Example:\n",
    "# Each test should verify one unit of behavior in isolation, without relying on shared state.\n",
    "\n",
    "def increment(x):\n",
    "    return x + 1\n",
    "\n",
    "def test_increment_isolated():\n",
    "    # Arrange\n",
    "    value = 3\n",
    "    # Act\n",
    "    result = increment(value)\n",
    "    # Assert\n",
    "    assert result == 4\n",
    "\n",
    "def test_increment_does_not_affect_other():\n",
    "    # Arrange\n",
    "    value = 10\n",
    "    # Act\n",
    "    result = increment(value)\n",
    "    # Assert\n",
    "    assert result == 11\n",
    "\n",
    "print(\"Both tests are isolated and do not depend on each other's state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0afd409",
   "metadata": {},
   "source": [
    "## Parametrized Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330b629",
   "metadata": {},
   "source": [
    "- run the same test logic multiple times with different inputs and expected outputs, without duplicating code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3815852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"a,b,expected\", [(1,2,3), (0,0,0), (-1,1,0)])\n",
    "def test_add(a,b,expected):\n",
    "    assert a+b == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e70f0e1",
   "metadata": {},
   "source": [
    "## Behavior-Driven Testing (BDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d12fe",
   "metadata": {},
   "source": [
    "## Testing Doubles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07007cd",
   "metadata": {},
   "source": [
    "- stand-ins for real objects used in testing to isolate the system under test (SUT). \n",
    "- They help simulate specific scenarios, avoid side effects (like network or database calls), and make tests deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b33bd9",
   "metadata": {},
   "source": [
    "| Type   | Description                                                                 | Example                                      |\n",
    "|--------|-----------------------------------------------------------------------------|----------------------------------------------|\n",
    "| Dummy  | Passed around but never actually used; only fills parameter lists.           | `DummyLogger()` with no functionality.       |\n",
    "| Stub   | Provides predefined responses to method calls; used to control test conditions. | A stub database that always returns a fixed user object. |\n",
    "| Fake   | Has working implementation but is simplified or not production-ready.        | In-memory database replacing a real one.     |\n",
    "| Spy    | Records information about how it was called (e.g., method name, arguments).  | Verifying that an email-sending function was called once. |\n",
    "| Mock   | Pre-programmed expectations about how it should be used; fails test if expectations aren’t met. | Mock API client expecting `.get(\"/user/1\")` to be called exactly once. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae8311",
   "metadata": {},
   "source": [
    "### Test Organization Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure large test suites for maintainability:\n",
    "# tests/\n",
    "#   ├── test_unit/\n",
    "#   │     ├── test_math.py\n",
    "#   │     └── test_utils.py\n",
    "#   ├── test_integration/\n",
    "#   │     └── test_api_endpoints.py\n",
    "#   ├── conftest.py   # pytest fixtures available project-wide\n",
    "#   └── data/         # golden files, mock data\n",
    "print(\"Organize tests by type and use fixtures for shared setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32b6bf",
   "metadata": {},
   "source": [
    "# Testing Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e0c00",
   "metadata": {},
   "source": [
    "# pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249de677",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    assert len(xs) > 0, \"mean() requires non-empty list\"\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "print(mean([1,2,3]))\n",
    "# mean([])  # would raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4079cf7",
   "metadata": {},
   "source": [
    "### Testing Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pal(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if s is a palindrome.\n",
    "\n",
    "    Examples:\n",
    "    >>> is_pal('racecar')\n",
    "    True\n",
    "    >>> is_pal('abc')\n",
    "    False\n",
    "    \"\"\"\n",
    "    return s == s[::-1]\n",
    "\n",
    "assert is_pal(\"madam\") is True\n",
    "assert is_pal(\"nope\") is False\n",
    "# To run doctests in a script:\n",
    "# if __name__ == \"__main__\":\n",
    "#     import doctest; doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541ff9b",
   "metadata": {},
   "source": [
    "### Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef0cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytest discovers functions named test_* in files/test modules.\n",
    "# Example tests (showing the style—run with `pytest -q` in a terminal):\n",
    "\n",
    "def add(a, b): return a + b\n",
    "\n",
    "def test_add_basic():\n",
    "    assert add(2, 3) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd5553",
   "metadata": {},
   "source": [
    "### Parametrized Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple inputs with one function (concept).\n",
    "#\n",
    "# import pytest\n",
    "# @pytest.mark.parametrize(\"nums,expected\", [\n",
    "#     ([1,2,3], 6),\n",
    "#     ([0,0,0], 0),\n",
    "#     ([-1,1], 0),\n",
    "# ])\n",
    "# def test_sum(nums, expected):\n",
    "#     assert sum(nums) == expected\n",
    "print(\"Use pytest.mark.parametrize for testing many cases compactly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b291696d",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Hypothesis is installed: `pip install hypothesis`\n",
    "# from hypothesis import given, strategies as st\n",
    "# @given(st.lists(st.integers()))\n",
    "# def test_reverse_reverse(xs):\n",
    "#     ys = list(reversed(list(reversed(xs))))\n",
    "#     assert ys == xs\n",
    "print(\"Property-based testing checks invariants across many random inputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939cd0b7",
   "metadata": {},
   "source": [
    "### Fuzz Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def reverse_twice(xs):\n",
    "    return list(reversed(list(reversed(xs))))\n",
    "\n",
    "for _ in range(5):\n",
    "    seq = [random.randint(-10,10) for _ in range(5)]\n",
    "    assert reverse_twice(seq) == seq\n",
    "\n",
    "print(\"Fuzz tests run random inputs to catch rare bugs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd38f68",
   "metadata": {},
   "source": [
    "### Snapshot / Golden File Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, tempfile, os\n",
    "\n",
    "def serialize(data):\n",
    "    return json.dumps(data, indent=2, sort_keys=True)\n",
    "\n",
    "expected_snapshot = '{\"a\": 1, \"b\": 2}'\n",
    "snapshot = serialize({\"a\": 1, \"b\": 2})\n",
    "\n",
    "assert snapshot.strip() == expected_snapshot.strip()\n",
    "print(\"Snapshot test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f26b1",
   "metadata": {},
   "source": [
    "### Monkeypatching (temporary overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a16e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using contextlib for a manual override.\n",
    "import contextlib\n",
    "\n",
    "def get_user():\n",
    "    import os\n",
    "    return os.getenv(\"USER\", \"unknown\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def mock_env(var, val):\n",
    "    import os\n",
    "    old = os.environ.get(var)\n",
    "    os.environ[var] = val\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if old is None:\n",
    "            del os.environ[var]\n",
    "        else:\n",
    "            os.environ[var] = old\n",
    "\n",
    "with mock_env(\"USER\", \"maverick\"):\n",
    "    assert get_user() == \"maverick\"\n",
    "\n",
    "print(\"Manual monkeypatch for env vars successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff662b78",
   "metadata": {},
   "source": [
    "### Integration Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c056e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate end-to-end flow using multiple functions together.\n",
    "def load_data():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "def process_data(xs):\n",
    "    return [x * 2 for x in xs]\n",
    "\n",
    "def main_pipeline():\n",
    "    return sum(process_data(load_data()))\n",
    "\n",
    "assert main_pipeline() == 12\n",
    "print(\"Integration test successful: pipeline verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ca18f",
   "metadata": {},
   "source": [
    "### Regression Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch re-introduced bugs by locking in a previously failing case.\n",
    "def divide(a, b):\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError\n",
    "    return a / b\n",
    "\n",
    "def test_divide_regression():\n",
    "    try:\n",
    "        divide(1, 0)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    else:\n",
    "        raise AssertionError(\"Expected ZeroDivisionError\")\n",
    "\n",
    "test_divide_regression()\n",
    "print(\"Regression test ensures old bug stays fixed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4b930",
   "metadata": {},
   "source": [
    "### Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixtures help share setup/teardown code cleanly.\n",
    "# (Example commented so notebook doesn’t error.)\n",
    "#\n",
    "# import pytest\n",
    "#\n",
    "# @pytest.fixture\n",
    "# def sample_data():\n",
    "#     return [1, 2, 3]\n",
    "#\n",
    "# def test_sum(sample_data):\n",
    "#     assert sum(sample_data) == 6\n",
    "#\n",
    "# Run with:\n",
    "# pytest -v\n",
    "print(\"pytest fixtures allow shared setup/teardown between tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1ee87",
   "metadata": {},
   "source": [
    "### Coverage & CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From terminal:\n",
    "#   pytest -q\n",
    "#   pytest -q -k \"keyword\"          # subset by name\n",
    "#   pytest -q -x                     # stop after first failure\n",
    "#   pytest --maxfail=1 --disable-warnings -q\n",
    "#   coverage run -m pytest && coverage html\n",
    "print(\"Run pytest/coverage from terminal; see comments for common commands.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
