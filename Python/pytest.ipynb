{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea385a75",
   "metadata": {},
   "source": [
    "## Basic Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648c64f",
   "metadata": {},
   "source": [
    "### Assertions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    assert len(xs) > 0, \"mean() requires non-empty list\"\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "print(mean([1,2,3]))\n",
    "# mean([])  # would raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d211b8b",
   "metadata": {},
   "source": [
    "### Testing Hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pal(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if s is a palindrome.\n",
    "\n",
    "    Examples:\n",
    "    >>> is_pal('racecar')\n",
    "    True\n",
    "    >>> is_pal('abc')\n",
    "    False\n",
    "    \"\"\"\n",
    "    return s == s[::-1]\n",
    "\n",
    "assert is_pal(\"madam\") is True\n",
    "assert is_pal(\"nope\") is False\n",
    "# To run doctests in a script:\n",
    "# if __name__ == \"__main__\":\n",
    "#     import doctest; doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26666d15",
   "metadata": {},
   "source": [
    "### Unit Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7196a3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytest discovers functions named test_* in files/test modules.\n",
    "# Example tests (showing the styleâ€”run with `pytest -q` in a terminal):\n",
    "\n",
    "def add(a, b): return a + b\n",
    "\n",
    "def test_add_basic():\n",
    "    assert add(2, 3) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c95a54",
   "metadata": {},
   "source": [
    "## Parametrized Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11702f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"a,b,expected\",\n",
    "    [\n",
    "        pytest.param(\n",
    "            datetime(2001, 12, 12), datetime(2001, 12, 11), timedelta(1), id=\"forward\"\n",
    "        ),\n",
    "        pytest.param(\n",
    "            datetime(2001, 12, 11), datetime(2001, 12, 12), timedelta(-1), id=\"backward\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "def test_timedistance_v3(a, b, expected):\n",
    "    diff = a - b\n",
    "    assert diff == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e09b578",
   "metadata": {},
   "source": [
    "## Markers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c1356",
   "metadata": {},
   "source": [
    "- Markers allow you to add metadata to your tests, enabling selective test execution and organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e8e100",
   "metadata": {},
   "source": [
    "### Built-in Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923af965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import sys\n",
    "\n",
    "# Skip a test\n",
    "@pytest.mark.skip(reason=\"Not implemented yet\")\n",
    "def test_feature_a():\n",
    "    assert False\n",
    "\n",
    "# Skip conditionally\n",
    "@pytest.mark.skipif(sys.version_info < (3, 10), reason=\"Requires Python 3.10+\")\n",
    "def test_feature_b():\n",
    "    assert True\n",
    "\n",
    "# Mark test as expected to fail\n",
    "@pytest.mark.xfail(reason=\"Known bug #123\")\n",
    "def test_buggy_feature():\n",
    "    assert 1 / 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f289c",
   "metadata": {},
   "source": [
    "### Custom Markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# Mark tests by category\n",
    "@pytest.mark.slow\n",
    "def test_database_query():\n",
    "    # Long-running test\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_api_endpoint():\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.unit\n",
    "def test_add():\n",
    "    assert 1 + 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8924f",
   "metadata": {},
   "source": [
    "### Running Tests by Marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa0b4cf",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run only slow tests\n",
    "pytest -m slow\n",
    "\n",
    "# Run everything except slow tests\n",
    "pytest -m \"not slow\"\n",
    "\n",
    "# Combine markers\n",
    "pytest -m \"integration and not slow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850a0a5",
   "metadata": {},
   "source": [
    "## Fixtures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529ae68",
   "metadata": {},
   "source": [
    "- provides a defined, reliable and consistent context for the tests. This could include environment (for example a database configured with known parameters) or content (such as a dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "class Fruit:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fruit():\n",
    "    return Fruit(\"apple\")\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def fruit_basket(my_fruit):\n",
    "    return [Fruit(\"banana\"), my_fruit]\n",
    "\n",
    "\n",
    "def test_my_fruit_in_basket(my_fruit, fruit_basket):\n",
    "    assert my_fruit in fruit_basket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ae966",
   "metadata": {},
   "source": [
    "### Fixture Scopes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af7b29",
   "metadata": {},
   "source": [
    "- Control how long a fixture lives and how often it's recreated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "# Function scope (default): created for each test\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def func_fixture():\n",
    "    return \"new for each test\"\n",
    "\n",
    "# Class scope: created once per test class\n",
    "@pytest.fixture(scope=\"class\")\n",
    "def class_fixture():\n",
    "    return \"shared across class\"\n",
    "\n",
    "# Module scope: created once per module\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def module_fixture():\n",
    "    return \"shared across module\"\n",
    "\n",
    "# Session scope: created once per test session\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def session_fixture():\n",
    "    return \"shared across all tests\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10918bb1",
   "metadata": {},
   "source": [
    "### Setup and Teardown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7bf38d",
   "metadata": {},
   "source": [
    "- Use `yield` for cleanup after tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import os\n",
    "\n",
    "@pytest.fixture\n",
    "def temp_file():\n",
    "    # Setup\n",
    "    file = open(\"test.txt\", \"w\")\n",
    "    file.write(\"test data\")\n",
    "    file.close()\n",
    "    \n",
    "    # Provide fixture value\n",
    "    yield \"test.txt\"\n",
    "    \n",
    "    # Teardown (runs after test completes)\n",
    "    import os\n",
    "    os.remove(\"test.txt\")\n",
    "\n",
    "def test_file_exists(temp_file):\n",
    "    assert os.path.exists(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bf1b7",
   "metadata": {},
   "source": [
    "### Autouse Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fa6394",
   "metadata": {},
   "source": [
    "- Automatically run for every test without explicit request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(autouse=True)\n",
    "def reset_state():\n",
    "    # Runs before every test automatically\n",
    "    global counter\n",
    "    counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff9c13",
   "metadata": {},
   "source": [
    "### Fixture Factories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25687e6a",
   "metadata": {},
   "source": [
    "- Return a function to create multiple instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bf285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def make_user():\n",
    "    def _make_user(name, age):\n",
    "        return {\"name\": name, \"age\": age}\n",
    "    return _make_user\n",
    "\n",
    "def test_users(make_user):\n",
    "    user1 = make_user(\"Alice\", 30)\n",
    "    user2 = make_user(\"Bob\", 25)\n",
    "    assert user1[\"age\"] > user2[\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a35e4",
   "metadata": {},
   "source": [
    "### Parametrized Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5476b",
   "metadata": {},
   "source": [
    "- Create multiple fixture variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a463d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture(params=[1, 2, 3])\n",
    "def number(request):\n",
    "    return request.param\n",
    "\n",
    "def test_number(number):\n",
    "    # This test runs 3 times with number=1, 2, 3\n",
    "    assert number > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00bf41",
   "metadata": {},
   "source": [
    "### Built-in Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97445b",
   "metadata": {},
   "source": [
    "- Pytest provides useful built-in fixtures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef46a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tmp_path(tmp_path):\n",
    "    # tmp_path: pathlib.Path to temp directory\n",
    "    file = tmp_path / \"test.txt\"\n",
    "    file.write_text(\"content\")\n",
    "    assert file.read_text() == \"content\"\n",
    "\n",
    "def test_capsys(capsys):\n",
    "    # capsys: capture stdout/stderr\n",
    "    print(\"hello\")\n",
    "    captured = capsys.readouterr()\n",
    "    assert captured.out == \"hello\\n\"\n",
    "\n",
    "def test_monkeypatch(monkeypatch):\n",
    "    # monkeypatch: modify objects/environment\n",
    "    monkeypatch.setenv(\"USER\", \"test_user\")\n",
    "    import os\n",
    "    assert os.getenv(\"USER\") == \"test_user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba7d3b1",
   "metadata": {},
   "source": [
    "## Hooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1946e",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ae18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conftest.py\n",
    "import pytest\n",
    "\n",
    "def pytest_configure(config):\n",
    "    \"\"\"Called after command line options are parsed.\"\"\"\n",
    "    config.addinivalue_line(\"markers\", \"smoke: mark test as smoke test\")\n",
    "\n",
    "def pytest_collection_modifyitems(config, items):\n",
    "    \"\"\"Modify collected test items.\"\"\"\n",
    "    for item in items:\n",
    "        if \"slow\" in item.keywords:\n",
    "            item.add_marker(pytest.mark.slow)\n",
    "\n",
    "def pytest_runtest_setup(item):\n",
    "    \"\"\"Called before running each test.\"\"\"\n",
    "    print(f\"\\nSetting up {item.name}\")\n",
    "\n",
    "def pytest_runtest_teardown(item):\n",
    "    \"\"\"Called after running each test.\"\"\"\n",
    "    print(f\"\\nTearing down {item.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db4669",
   "metadata": {},
   "source": [
    "## Coverage & CLI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From terminal:\n",
    "#   pytest -q\n",
    "#   pytest -q -k \"keyword\"          # subset by name\n",
    "#   pytest -q -x                     # stop after first failure\n",
    "#   pytest --maxfail=1 --disable-warnings -q\n",
    "#   coverage run -m pytest && coverage html\n",
    "print(\"Run pytest/coverage from terminal; see comments for common commands.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
