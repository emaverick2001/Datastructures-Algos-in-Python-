{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2dc17b",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6661507",
   "metadata": {},
   "source": [
    "## Testing Patterns / Principles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19562fdf",
   "metadata": {},
   "source": [
    "### TDD Principles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a1204",
   "metadata": {},
   "source": [
    "- Red → Green → Refactor\n",
    "  - write a failing test, make it pass, then clean up\n",
    "- Test small units first; keep tests deterministic (seed RNG)\n",
    "- Isolate side effects (I/O, network) via dependency injection or mocks\n",
    "- Favor fast tests; run them in CI on every push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9619b7",
   "metadata": {},
   "source": [
    "### AAA: Arrange-Act-Assert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648c304",
   "metadata": {},
   "source": [
    "- Keep each phase visually separated (blank lines help)\n",
    "- Assert invariants (shape, monotonicity, conservation) not just values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea79d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(xs):\n",
    "    total = sum(xs)\n",
    "    return [x/total for x in xs] if total else xs\n",
    "\n",
    "# Arrange\n",
    "xs = [2, 2, 6]\n",
    "# Act\n",
    "ys = normalize(xs)\n",
    "# Assert\n",
    "assert abs(sum(ys) - 1.0) < 1e-9\n",
    "assert len(ys) == len(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8ddb5",
   "metadata": {},
   "source": [
    "### Clean Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategies to isolate tests:\n",
    "# - Use tmp_path / tempfile for safe file writes\n",
    "# - Mock environment variables\n",
    "# - Reset global state (random.seed, numpy seed)\n",
    "# - Close file handles, sockets, or DB connections after each test\n",
    "print(\"Always clean up global or I/O state between tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c29c90",
   "metadata": {},
   "source": [
    "### Test Organization Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure large test suites for maintainability:\n",
    "# tests/\n",
    "#   ├── test_unit/\n",
    "#   │     ├── test_math.py\n",
    "#   │     └── test_utils.py\n",
    "#   ├── test_integration/\n",
    "#   │     └── test_api_endpoints.py\n",
    "#   ├── conftest.py   # pytest fixtures available project-wide\n",
    "#   └── data/         # golden files, mock data\n",
    "print(\"Organize tests by type and use fixtures for shared setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b720c4",
   "metadata": {},
   "source": [
    "## pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf75d71",
   "metadata": {},
   "source": [
    "### Assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cff330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    assert len(xs) > 0, \"mean() requires non-empty list\"\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "print(mean([1,2,3]))\n",
    "# mean([])  # would raise AssertionError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc64d2",
   "metadata": {},
   "source": [
    "### Testing Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pal(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True if s is a palindrome.\n",
    "\n",
    "    Examples:\n",
    "    >>> is_pal('racecar')\n",
    "    True\n",
    "    >>> is_pal('abc')\n",
    "    False\n",
    "    \"\"\"\n",
    "    return s == s[::-1]\n",
    "\n",
    "assert is_pal(\"madam\") is True\n",
    "assert is_pal(\"nope\") is False\n",
    "# To run doctests in a script:\n",
    "# if __name__ == \"__main__\":\n",
    "#     import doctest; doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e47c2",
   "metadata": {},
   "source": [
    "### Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4abf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytest discovers functions named test_* in files/test modules.\n",
    "# Example tests (showing the style—run with `pytest -q` in a terminal):\n",
    "\n",
    "def add(a, b): return a + b\n",
    "\n",
    "def test_add_basic():\n",
    "    assert add(2, 3) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81302be6",
   "metadata": {},
   "source": [
    "### Parametrized Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6330f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple inputs with one function (concept).\n",
    "#\n",
    "# import pytest\n",
    "# @pytest.mark.parametrize(\"nums,expected\", [\n",
    "#     ([1,2,3], 6),\n",
    "#     ([0,0,0], 0),\n",
    "#     ([-1,1], 0),\n",
    "# ])\n",
    "# def test_sum(nums, expected):\n",
    "#     assert sum(nums) == expected\n",
    "print(\"Use pytest.mark.parametrize for testing many cases compactly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959da658",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613e72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Hypothesis is installed: `pip install hypothesis`\n",
    "# from hypothesis import given, strategies as st\n",
    "# @given(st.lists(st.integers()))\n",
    "# def test_reverse_reverse(xs):\n",
    "#     ys = list(reversed(list(reversed(xs))))\n",
    "#     assert ys == xs\n",
    "print(\"Property-based testing checks invariants across many random inputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b40b",
   "metadata": {},
   "source": [
    "### Fuzz Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4eef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def reverse_twice(xs):\n",
    "    return list(reversed(list(reversed(xs))))\n",
    "\n",
    "for _ in range(5):\n",
    "    seq = [random.randint(-10,10) for _ in range(5)]\n",
    "    assert reverse_twice(seq) == seq\n",
    "\n",
    "print(\"Fuzz tests run random inputs to catch rare bugs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89411a6b",
   "metadata": {},
   "source": [
    "### Snapshot / Golden File Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b1b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, tempfile, os\n",
    "\n",
    "def serialize(data):\n",
    "    return json.dumps(data, indent=2, sort_keys=True)\n",
    "\n",
    "expected_snapshot = '{\"a\": 1, \"b\": 2}'\n",
    "snapshot = serialize({\"a\": 1, \"b\": 2})\n",
    "\n",
    "assert snapshot.strip() == expected_snapshot.strip()\n",
    "print(\"Snapshot test passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f7718",
   "metadata": {},
   "source": [
    "### Monkeypatching (temporary overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using contextlib for a manual override.\n",
    "import contextlib\n",
    "\n",
    "def get_user():\n",
    "    import os\n",
    "    return os.getenv(\"USER\", \"unknown\")\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def mock_env(var, val):\n",
    "    import os\n",
    "    old = os.environ.get(var)\n",
    "    os.environ[var] = val\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        if old is None:\n",
    "            del os.environ[var]\n",
    "        else:\n",
    "            os.environ[var] = old\n",
    "\n",
    "with mock_env(\"USER\", \"maverick\"):\n",
    "    assert get_user() == \"maverick\"\n",
    "\n",
    "print(\"Manual monkeypatch for env vars successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b26554",
   "metadata": {},
   "source": [
    "### Integration Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57b00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate end-to-end flow using multiple functions together.\n",
    "def load_data():\n",
    "    return [1, 2, 3]\n",
    "\n",
    "def process_data(xs):\n",
    "    return [x * 2 for x in xs]\n",
    "\n",
    "def main_pipeline():\n",
    "    return sum(process_data(load_data()))\n",
    "\n",
    "assert main_pipeline() == 12\n",
    "print(\"Integration test successful: pipeline verified.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4b366",
   "metadata": {},
   "source": [
    "### Regression Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c595ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch re-introduced bugs by locking in a previously failing case.\n",
    "def divide(a, b):\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError\n",
    "    return a / b\n",
    "\n",
    "def test_divide_regression():\n",
    "    try:\n",
    "        divide(1, 0)\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    else:\n",
    "        raise AssertionError(\"Expected ZeroDivisionError\")\n",
    "\n",
    "test_divide_regression()\n",
    "print(\"Regression test ensures old bug stays fixed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d7c36",
   "metadata": {},
   "source": [
    "### Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixtures help share setup/teardown code cleanly.\n",
    "# (Example commented so notebook doesn’t error.)\n",
    "#\n",
    "# import pytest\n",
    "#\n",
    "# @pytest.fixture\n",
    "# def sample_data():\n",
    "#     return [1, 2, 3]\n",
    "#\n",
    "# def test_sum(sample_data):\n",
    "#     assert sum(sample_data) == 6\n",
    "#\n",
    "# Run with:\n",
    "# pytest -v\n",
    "print(\"pytest fixtures allow shared setup/teardown between tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac97d15",
   "metadata": {},
   "source": [
    "### Coverage & CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54602a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From terminal:\n",
    "#   pytest -q\n",
    "#   pytest -q -k \"keyword\"          # subset by name\n",
    "#   pytest -q -x                     # stop after first failure\n",
    "#   pytest --maxfail=1 --disable-warnings -q\n",
    "#   coverage run -m pytest && coverage html\n",
    "print(\"Run pytest/coverage from terminal; see comments for common commands.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aec9ca",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59b7e8f",
   "metadata": {},
   "source": [
    "## breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buggy_sum(xs):\n",
    "    total = 0\n",
    "    for x in xs:\n",
    "        # if x is None: breakpoint()  # uncomment to drop into pdb\n",
    "        total += x\n",
    "    return total\n",
    "\n",
    "print(buggy_sum([1,2,3]))\n",
    "# buggy_sum([1,None,3])  # uncomment with breakpoint() to inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69442f55",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(message)s\")\n",
    "\n",
    "def divide(a, b):\n",
    "    logging.debug(\"divide called with a=%s b=%s\", a, b)\n",
    "    if b == 0:\n",
    "        logging.error(\"division by zero\")\n",
    "        raise ZeroDivisionError(\"b must not be zero\")\n",
    "    res = a / b\n",
    "    logging.info(\"result=%s\", res)\n",
    "    return res\n",
    "\n",
    "print(divide(6, 3))\n",
    "# divide(1, 0)  # would log error and raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37b07d",
   "metadata": {},
   "source": [
    "## warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "def old_api():\n",
    "    warnings.warn(\"old_api is deprecated; use new_api\", DeprecationWarning, stacklevel=2)\n",
    "    return 42\n",
    "\n",
    "# By default, DeprecationWarning may be hidden. Make it visible:\n",
    "warnings.simplefilter(\"default\", DeprecationWarning)\n",
    "print(old_api())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeba8e1",
   "metadata": {},
   "source": [
    "## Tracing Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f69c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_int(s: str, default=None):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError as e:\n",
    "        # attach context, keep original traceback\n",
    "        raise ValueError(f\"Cannot parse int from {s!r}\") from e\n",
    "\n",
    "print(safe_parse_int(\"10\"))\n",
    "# safe_parse_int(\"ten\")  # would raise with helpful message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a194d1",
   "metadata": {},
   "source": [
    "## Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ddd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def slow():\n",
    "    return sum(i*i for i in range(10_000))\n",
    "\n",
    "print(timeit.timeit(slow, number=100))  # seconds for 100 runs\n",
    "# In notebooks you can also use:\n",
    "# %timeit slow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60633937",
   "metadata": {},
   "source": [
    "## Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711366dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile, pstats, io\n",
    "\n",
    "def work(n=30_000):\n",
    "    s = 0\n",
    "    for i in range(n):\n",
    "        s += (i % 7) * (i % 11)\n",
    "    return s\n",
    "\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "_ = work()\n",
    "pr.disable()\n",
    "\n",
    "s = io.StringIO()\n",
    "ps = pstats.Stats(pr, stream=s).sort_stats(\"cumtime\")\n",
    "ps.print_stats(10)           # top 10 entries\n",
    "print(s.getvalue().splitlines()[0:15])  # show first few lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a0cd6",
   "metadata": {},
   "source": [
    "## Determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "vals = [random.randint(1, 3) for _ in range(5)]\n",
    "print(vals)  # stable across runs when seeded"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
